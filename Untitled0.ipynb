{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOvQuLKvwSvd46Eal65Pdrq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "fMq_SVegl2R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "gPyyHE7Pl7aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "k1uUpnydp0dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip_data(filename):\n",
        "  zip_ref = zipfile.ZipFile(filename)\n",
        "  zip_ref.extractall()\n",
        "  zip_ref.close()"
      ],
      "metadata": {
        "id": "ZVF7gwUrqGo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_curves(history):\n",
        "  loss = history.history[\"loss\"]\n",
        "  val_loss = history.history[\"val_loss\"]\n",
        "  accuracy = history.history[\"accuracy\"]\n",
        "  val_accuracy = history.history[\"val_accuracy\"]\n",
        "  epochs = range(len(history.history[\"loss\"]))\n",
        "  plt.plot(epochs, loss, label=\"training loss\")\n",
        "  plt.plot(epochs, val_loss, label=\"validation loss\")\n",
        "  plt.title(\"LOSS\")\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.plot(epochs, accuracy, label=\"training accuracy\")\n",
        "  plt.plot(epochs, val_accuracy, label=\"validation accuracy\")\n",
        "  plt.title(\"ACCURACY\")\n",
        "  plt.xlable(\"epochs\")\n",
        "  plt.legend()\n"
      ],
      "metadata": {
        "id": "YZoO-_7sqcxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\""
      ],
      "metadata": {
        "id": "HGmpa8pYs99B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unzip_data(\"/content/nlp_getting_started.zip\")"
      ],
      "metadata": {
        "id": "8pqkl3cpvGoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "S2QM7l-hvZxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train.csv\")"
      ],
      "metadata": {
        "id": "1a9C79EixPPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(5)"
      ],
      "metadata": {
        "id": "zFk89wrnxVUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"test.csv\")\n",
        "test.head(5)"
      ],
      "metadata": {
        "id": "GxqYE0xWxXfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "id": "S8DW3Ol9yROn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train.loc[0][\"id\"]), type(train.loc[0]), type(train)"
      ],
      "metadata": {
        "id": "CjLjx8lWyT6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.loc[0:3]"
      ],
      "metadata": {
        "id": "mMmwQ_ZCycLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.iloc[0:3]"
      ],
      "metadata": {
        "id": "70mJIXXW0Aj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.loc[(train.id==1)]"
      ],
      "metadata": {
        "id": "Wbnor9IH0D7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.loc[train.id==1]"
      ],
      "metadata": {
        "id": "iwg8Wl900Kui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.iloc[[0,3, 2]]"
      ],
      "metadata": {
        "id": "2IDKRlXt0wlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.loc[[0,2]]"
      ],
      "metadata": {
        "id": "Df1fbiCi1Can"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.loc[0][\"text\"], train.loc[0][\"target\"]"
      ],
      "metadata": {
        "id": "uUhv2O1R1RUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.loc[(train.target==0)]"
      ],
      "metadata": {
        "id": "v4c38Syk1ppY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.loc[(train[\"id\"]==23)] # train.iloc[(train.id==23)] gives error coz ilocation does not use boolean based index"
      ],
      "metadata": {
        "id": "b4fUDt1px8ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From above, we can say that target 0 ==> normal tweet & 1 ==> disaster tweet"
      ],
      "metadata": {
        "id": "UexLx9mU17BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_shuffled = train.sample(frac=1, random_state = 42)"
      ],
      "metadata": {
        "id": "hPcTXpKB1zvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_shuffled.head(5)"
      ],
      "metadata": {
        "id": "OdsLLcpN-41y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[\"target\"].value_counts()"
      ],
      "metadata": {
        "id": "B3L_TgP2BrX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train), len(train.loc[train[\"target\"]==0])/len(train), len(train.loc[(train[\"target\"]==1)])/len(train)"
      ],
      "metadata": {
        "id": "QSQznQ9tCaDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many samples in total?\n",
        "print(f\"Total Train samples:{len(train)}\") # print(\"total: \",len(train))\n",
        "print(f\"Total test samples: {len(test)}\")\n",
        "print(f\"Total samples: {len(train) + len(test)}\")"
      ],
      "metadata": {
        "id": "pj241jTiCrgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.sample(frac=0.25).head(10)"
      ],
      "metadata": {
        "id": "W2DibtxiCzeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's visualize random training samples\n",
        "import random\n",
        "random_idx = random.randint(0, len(train)-5) # generate a random integer within the range (0,len(train)-5)"
      ],
      "metadata": {
        "id": "vLe8F4u_EJLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_idx"
      ],
      "metadata": {
        "id": "qYDfSXfYH0bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in train_shuffled[[\"text\",\"target\"]][random_idx:random_idx+5].itertuples():\n",
        "  _, text, target = row\n",
        "  print(f\"{target} (real disaster)\" if target > 0 else f\"{target} (not real disaster)\")\n",
        "  print(f\"Text: {text}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "kTpmyMcDH_cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_shuffled[['text','target']]), len(train_shuffled[['text','target']])"
      ],
      "metadata": {
        "id": "jRtFElLwIPrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_shuffled[['text','target']][0:2] # train[0:2]"
      ],
      "metadata": {
        "id": "02JEL4iSIYxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "PV3S9WeVIxSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, train_labels, val_labels = train_test_split(np.array(train_shuffled[\"text\"]),\n",
        "                                                                  np.array(train_shuffled[\"target\"]), \n",
        "                                                                  test_size=0.1, \n",
        "                                                                  random_state=42)\n",
        "\n",
        "# train_data, val_data, train_labels, val_labels = train_test_split(train_shuffled[\"text\"].to_numpy(),\n",
        "#                                                                    train_shuffled[\"target\"].to_numpy(),\n",
        "#                                                                    test_size=0.1,\n",
        "#                                                                    random_state = 42)"
      ],
      "metadata": {
        "id": "x3-ffCRng5-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "d1ZC369Eg49n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data),len(val_data), len(train_labels), len(val_labels)"
      ],
      "metadata": {
        "id": "3EFSiSLsg4pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_data), sum([len(sent.split()) for sent in train_data])"
      ],
      "metadata": {
        "id": "q_6ZT5ftvbnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization"
      ],
      "metadata": {
        "id": "NXSV0oFdg4UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text_vectorizer = TextVectorization(max_tokens=None,\n",
        "#                                     standardize=\"lower_and_strip_punctuation\",\n",
        "#                                     split=\"whitespace\",\n",
        "#                                     ngrams=None,\n",
        "#                                     output_mode=\"int\",\n",
        "#                                     output_sequence_length=None)"
      ],
      "metadata": {
        "id": "wALB2DP0g39L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting max_tokens\n",
        "max_vocab_length = 10000\n",
        "\n",
        "# setting output_sequence_length to the average no. of words per tweet\n",
        "max_length = round(sum([len(sent.split()) for sent in train_data])/ len(train_data)) # list comprehension"
      ],
      "metadata": {
        "id": "cPI6HL7vg34E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
        "                                   output_mode=\"int\",\n",
        "                                   output_sequence_length=max_length)"
      ],
      "metadata": {
        "id": "94zQoptSxvgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorizer.adapt(train_data)\n",
        "# During adapt(), the layer will build a vocabulary of all string tokens seen in the dataset,\n",
        "# sorted by occurance count, with ties broken by sort order of the tokens (high to low)."
      ],
      "metadata": {
        "id": "O3w0FNdpxvcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's test our text vectorizer\n",
        "text_vectorizer([\"I am enough!\"])"
      ],
      "metadata": {
        "id": "Y1h27EonxvL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_sent = random.choice(train_data)\n",
        "text_vectorizer(random_sent)"
      ],
      "metadata": {
        "id": "IYs4VR2Fzd54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_vocab = text_vectorizer.get_vocabulary()\n",
        "\n",
        "top_5_words = words_in_vocab[:5]\n",
        "bottom_5_words = words_in_vocab[-5:]\n",
        "print(f\"Total words in vocabualry: {len(words_in_vocab)}\")\n",
        "print(f\"five most common words: {top_5_words}\")\n",
        "print(f\"five most uncommon words: {bottom_5_words}\")"
      ],
      "metadata": {
        "id": "iwGnPAkA1jMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "from tensorflow.keras import layers\n",
        "embedding = layers.Embedding(input_dim=max_vocab_length,\n",
        "                                              output_dim=128,\n",
        "                                              embeddings_initializer=\"uniform\",\n",
        "                                              input_length=max_length,\n",
        "                                              name=\"embedding_1\")"
      ],
      "metadata": {
        "id": "umbD327g2Imc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding"
      ],
      "metadata": {
        "id": "0RR1qt4J4jgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_sentence = random.choice(train_data)\n",
        "print(\"Original sentence: \",random_sentence)\n",
        "print(\"Numerical encoded sentence: \",text_vectorizer([random_sentence]))\n",
        "print(\"Embedded version: \")\n",
        "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
        "print(sample_embed)"
      ],
      "metadata": {
        "id": "1jUOHlTz4jXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_embed[0][0]"
      ],
      "metadata": {
        "id": "Wi_xKtU2GHgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "-KrHPTctHmwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JzeFqsxyGHaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ACa95ZQ74jR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2PMDh74V4jGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQTGG7Nz4jAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKZ2X6Rd4i2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UqwXtfB-4iut"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}